# 研究者向け分析 Agent 基本設計書

このドキュメントは、ローカルで動作する研究者向け分析 Agent の基本設計をまとめたものです。本システムは **web‑search‑mcp**、**container‑use**、および **Ollama 上の gpt‑oss:20b** を活用し、LangGraph を利用してステートフルなワークフローを構築します。クラウドサービスを一切利用せず、日本語または英語で対話できることを前提としています。

## 1. 目的と概要

現代の研究者や開発者は大量の文献や技術情報を効率的に収集・整理し、さらに理解した内容を自分の研究に適用する必要があります。本システムはそのプロセスを支援するため、ユーザーからの質問に対して関連情報の収集、外部実行環境での処理、内容の検証を繰り返し、最終的にマークダウンや PDF 形式のレポートとして出力します。主要な特長は次の通りです。

- **完全ローカルでの実行**：Web 検索用 MCP サーバーを除き、データやプロンプトは外部クラウドに送信しません。
- **調査と検証のループ**：一次情報を収集し、コンテナ環境で処理して暫定結果を生成した後、追加検索や再検証を行い、矛盾や誤りを減らします。
- **LangGraph によるワークフロー構築**：複数のノードを持つステートフルなグラフとして Agent を実装し、耐障害性の高い長時間実行を実現します【76357785828370†L105-L117】【76357785828370†L156-L167】。

## 2. 技術選定と前提条件

### 2.1 LangGraph

LangGraph は長時間稼働するエージェントやワークフローのための低レベルオーケストレーションフレームワークであり、耐障害性・ストリーミング・人間の介入などの機能を提供します【76357785828370†L105-L117】。各処理は **ノード**として定義され、ノード間の遷移や状態管理を明示的に記述できます【862685349901645†L121-L127】。ステップを細かく分割し、それぞれに必要な処理と共有状態を定義することで、複雑なエージェントを構築できます【862685349901645†L156-L161】。LangGraph は durable execution・human‑in‑the‑loop・メモリなど複数の利点を提供し【76357785828370†L156-L168】、状態を保持しながら長時間動作するシステムに適しています。

### 2.2 Web Search MCP サーバー

**web‑search‑mcp** はローカル LLM 向けの MCP サーバーで、完全な Web 検索およびページ内容の抽出を行うツール群を提供します【614118497951608†L2-L11】。主なツールは以下の3つです。

1. **full‑web‑search**：検索語に対し上位結果を取得し、それぞれのページの全文を抽出する総合検索ツール【614118497951608†L2-L11】。入力パラメータには検索語と結果数があり、結果にはタイトル・URL・説明・全文が含まれます【614118497951608†L12-L20】。
2. **get‑web‑search‑summaries**：検索語に対して要約のみを返す軽量検索ツール【614118497951608†L29-L37】。
3. **get‑single‑web‑page‑content**：特定の URL から本文を抽出するツール【614118497951608†L45-L55】。

LobeHub の解説では、この MCP サーバーは Google 検索を試し、結果が得られない場合は DuckDuckGo にフォールバックするマルチエンジン検索機能や、検索結果の全文抽出機能を持つことが紹介されています【382505332529582†L71-L87】。MCP プロトコルに準拠しているため、AI アシスタントとの統合も容易です【382505332529582†L71-L87】。

### 2.3 Container Use

**container‑use** は AI エージェントのための分離された開発環境を提供するツールで、各エージェントを独立したコンテナと Git ブランチに割り当てます【448948602355290†L47-L73】。これにより複数エージェントが同じコードベースで安全に並行作業でき、失敗時にはすぐに破棄できます【448948602355290†L55-L58】。さらに、実行履歴やログの閲覧、コンテナに直接アクセスして介入する機能があり【448948602355290†L60-L68】、標準的な Git ワークフロー（`git checkout` など）で各ブランチを確認できます【448948602355290†L70-L73】。InfoQ の記事では、container‑use が各エージェントに対してコンテナ化されたサンドボックスと Git worktree を自動的に生成し、並列で衝突のないワークフローを提供することが説明されています【932081450083569†L257-L283】。

### 2.4 LLM と Ollama

Ollama はローカル実行用に最適化された LLM フレームワークです。本システムでは **gpt‑oss:20b** をデフォルトモデルとして使用します。gpt‑oss シリーズは OpenAI が公開したオープンウエイトモデルで、強力な推論能力とエージェント用途に向いた機能を備えています【65291264465621†L10-L17】。gpt‑oss:20b は 21B パラメータで 3.6B アクティブパラメータを持ち、低レイテンシかつローカル環境での利用に適します【65291264465621†L10-L17】。ハイライトとして、推論の深さを調整できる **configurable reasoning**、完全なチェインオブソートアクセス、ファインチューニング対応、ネイティブな関数呼び出しやツール実行能力などがあります【65291264465621†L27-L38】。Ollama では `ollama pull gpt-oss:20b` と `ollama run gpt-oss:20b` を実行するだけでモデルをダウンロードして起動できることが示されています【65291264465621†L92-L96】。他のローカルモデルへの切り替えも同様のコマンドで可能です。

### 2.5 その他の前提条件

* **プログラミング言語**：実装は Python を用います。
* **LangGraph 利用**：ノードを Python 関数として記述し、状態を `TypedDict` などで定義します。各ノードはインプット状態からアウトプット状態への変換を返す形式とし、デコレータまたは `StateGraph` API でワークフローを構築します。
* **使用言語**：ユーザーとの対話およびレポート出力は日本語または英語のみに対応します。ユーザー入力を判定して適切な言語で回答するモジュールを実装します。
* **禁止事項**：OpenAI、Claude、Google、Azure 等のクラウドモデルは使用せず、すべての計算をローカルで行います。機密データや外部サービスへの送信を防ぎます。
* **履歴管理**：各セッションの入力・検索結果・最終レポートを Markdown (`.md`) ファイルとして保存し、必要に応じて PDF 形式へ変換します。

## 3. 全体アーキテクチャ

以下の図はシステムの概要アーキテクチャを示します。各処理は LangGraph のノードとして実装され、状態を共有しながら順次実行されます。

![Architecture Diagram](architecture_diagram.png)

### 3.1 主なコンポーネント

| モジュール | 役割 | 主要技術 |
|---|---|---|
| **ユーザーインタフェース** | ユーザーからのプロンプト入力を受け取り、結果を表示する。CLI または Web UI として実装可能。 | Python CLI / Webフレームワーク |
| **LangGraph オーケストレータ** | ノードの追加・遷移・状態管理を担当し、耐障害性の高いワークフローを構築する【76357785828370†L105-L117】。| LangGraph |
| **検索ノード** | web‑search‑mcp を呼び出し、`full-web-search` などのツールで関連情報を収集する。必要に応じて再試行やパラメータ調整を行う。【614118497951608†L2-L11】 | web‑search‑mcp |
| **処理ノード** | 収集したデータを分析・前処理する。大きな文書のパースや要約、コードの実行などをコンテナ環境で行い、他のノードに影響しないよう分離する【448948602355290†L55-L58】【448948602355290†L60-L68】。| container‑use |
| **LLM 推論ノード** | Ollama から gpt‑oss:20b を呼び出し、要約や分析、暫定回答生成などの推論を実行する。必要に応じて複数モデルを切り替える。| Ollama + gpt‑oss シリーズ |
| **検証ループノード** | 暫定回答を検証するため再度検索や分析を行う。矛盾点や不足している情報を検出し、必要であれば追加の検索ノードや処理ノードを呼び出す。| LangGraph ループ機構 |
| **レポート生成ノード** | LLM からの最終的なアウトプットを整形し、指定された言語でマークダウンレポートを生成する。PDF への変換は Python ライブラリ（例えば `markdown` + `reportlab`）を使用する。| Python, gpt‑oss |
| **履歴管理モジュール** | 全ての検索結果、処理ステップ、レポートを日時付きフォルダーに Markdown 形式で保存し、セッションの再現性を確保する。| ファイルシステム |
| **モデル管理モジュール** | gpt‑oss:20b を標準モデルとして設定し、設定ファイルや CLI オプションで他のローカルモデルに切り替えられるようにする。| Ollama CLI |

## 4. LangGraph ワークフロー設計

### 4.1 ノード構成

ワークフローは以下のノードで構成されます。各ノードは Python 関数として定義し、入力状態 (`state`) と出力状態の差分 (`dict`) を返します。LangGraph の設計原則では、状態は生データを保持し、各ノードで必要なフォーマットに整形します【862685349901645†L263-L297】。

1. **InputNode**：ユーザーからプロンプトと設定（言語指定・モデル指定など）を受け取り、状態に保存する。
2. **SearchNode**：`full-web-search` または `get-web-search-summaries` を呼び出し、クエリに応じた結果と全文を取得する。検索結果は状態に保存し、文書のメタデータと URL を保持する【614118497951608†L2-L11】【382505332529582†L71-L87】。
3. **ProcessingNode**：取得したページをコンテナ内で処理する。具体的には、PDF や HTML をパースしてテキスト化し、不要な要素を除去する。コンテナ化により他のノードから隔離され、安全な実行環境を提供する【448948602355290†L55-L58】【448948602355290†L60-L68】。
4. **LLMNode**：gpt‑oss で処理済みデータを要約・分析し、暫定的な回答を生成する。LangGraph 内ではツール呼び出し機能を利用し、結果を状態に保存する。モデル切り替えは設定値に従い、Ollama CLI で適切なモデルを呼び出す。
5. **VerificationNode**：暫定回答が適切かどうかを判定する。具体的には、回答内の主張や数値を抽出して再度検索し、矛盾がないかを確認する。矛盾が検出された場合は SearchNode・ProcessingNode・LLMNode を再度呼び出すループ処理を行う。
6. **ReportNode**：検証済みの最終内容をまとめ、指定言語で Markdown レポートを生成する。Markdown ファイルは履歴管理モジュールに保存し、必要に応じて PDF へ変換する。

### 4.2 状態設計

状態 (`state`) は次のような `TypedDict` として定義し、LangGraph の全ノードで共有します。

```python
class AgentState(TypedDict):
    messages: list[dict]             # ユーザーとエージェントの対話履歴
    query: str                       # 現在の検索クエリ
    search_results: list[dict]       # Web 検索から得られた結果
    processed_data: list[dict]       # コンテナ内で処理したデータ
    provisional_answer: str          # 暫定回答
    final_report: str                # 最終レポート（Markdown）
    language: str                    # ‘ja’ または ‘en’
    model_name: str                  # 使用するモデル名（デフォルトは gpt-oss:20b）
    history_path: str                # 履歴ファイルの保存先
```

状態には長期的に必要なデータのみを保存し、その他の変換やフォーマットは各ノードで行います【862685349901645†L263-L297】。

## 5. 処理フロー

1. **プロンプト受信**：ユーザーがプロンプトと必要設定（言語やモデル選択）を入力します。入力は `InputNode` に渡され、状態が初期化されます。
2. **検索**：`SearchNode` が web‑search‑mcp の `full-web-search` や `get-web-search-summaries` を用いて関連情報を取得します【614118497951608†L2-L11】。取得した結果は `search_results` に保存します。必要に応じて `limit` パラメータを調整し、追加検索を行います【614118497951608†L12-L20】。
3. **データ処理**：`ProcessingNode` はコンテナ内でページ内容をパースし、図表の抽出や OCR を実行します。コンテナは各タスクごとに分離され、Git ワークツリーにより他エージェントと衝突しない環境が提供されます【448948602355290†L55-L58】。
4. **推論**：`LLMNode` が Ollama へ問い合わせ、gpt‑oss:20b を利用して要約や分析を実行します。モデルにツール呼び出しが必要な場合は Harmony 形式で出力させ、LangGraph から結果を受け取ります【65291264465621†L27-L38】。
5. **検証ループ**：`VerificationNode` が暫定回答を吟味し、重要な事実や数値を抽出して追加検索します。矛盾が見つかった場合、状態を更新して 2〜4 を再度実行します。このループにより誤情報や情報漏れを減らします。
6. **レポート生成**：検証済みの内容を `ReportNode` が Markdown 形式のレポートとして生成し、必要に応じて PDF に変換します。レポートは `history_path` に保存され、ユーザーに提示されます。

## 6. 履歴管理

各セッションには一意のタイムスタンプまたは UUID を付与し、`session_<timestamp>` ディレクトリ内に以下のファイルを保存します。

- `input.md`：ユーザーのプロンプトと設定を記録。
- `search_results.md`：web‑search‑mcp から得た検索結果を記録。
- `processed_data.md`：コンテナ内での処理結果やログを記録。
- `report.md`：最終的なレポート。報告後 PDF も生成する場合は `report.pdf` を保存。
- `state.json`：セッション終了時の状態を JSON 形式で保存し、再開やデバッグに利用。

履歴はバージョン管理の対象ではなく、ローカルファイルとして保存します。必要に応じて古い履歴を削除するクリーンアップ機能を実装します。

## 7. モデル切替機能

デフォルトは `gpt‑oss:20b` ですが、他の gpt‑oss 系モデル（例: gpt‑oss:120b）や別のローカルモデルを利用する可能性があります。モデル名は設定ファイルまたは CLI 引数から受け取り、LLM ノードで参照します。モデルの変更は Ollama CLI で `ollama pull <model>`・`ollama run <model>` を実行することで行えます【65291264465621†L92-L96】。LangGraph の状態には `model_name` フィールドを設け、ノードごとに異なるモデルを使用したい場合でも柔軟に対応できます。

## 8. 非機能要件

- **性能とスケーラビリティ**：並列検索や並列コンテナ処理を行うことで、大量の文献を扱う場合でも時間を短縮します。container‑use は各エージェントに独立したコンテナを割り当て、衝突のない並行実行を保証します【932081450083569†L257-L283】。
- **堅牢性**：LangGraph の durable execution 機能により、途中で失敗しても状態を保持したまま再開できます【76357785828370†L156-L167】。検索失敗時には検索エンジンの切り替えやリトライ戦略を実装します【382505332529582†L71-L87】。
- **プライバシー**：全ての処理はローカル環境で行い、検索以外のデータを外部に送信しません。Container 内でコードを実行することで、ユーザーのデータやコードが隔離されます。
- **拡張性**：モジュール構造と LangGraph により、将来的に新しいツール（例えば学術データベース API や数式処理エンジン）を追加する場合でも、ノードを追加してグラフに結合するだけで対応できます。


