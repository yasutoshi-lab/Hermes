# 文献サマライズ＆分析エージェントの基本設計書

## 0. システム名

**Hermes**

## 1. 目的と背景

このシステムは開発者や研究者が論文や技術文書を手元の環境で要約・分析するための**研究支援エージェント**です。利用者の個人情報や未公開資料を外部へ送信せず、ローカルで安全に処理することを重視します。OpenManus のようなエージェントフレームワークを参考にしながらも、以下のOSSコンポーネントを主要な構成要素として組み合わせます。

- **Web‑Search‑MCP** – MCP プロトコル準拠の Web 検索サーバ。Bing→Brave→DuckDuckGo の順にマルチエンジン検索を行い、ページ全文の抽出や軽量サマリーなど 3 種類の検索ツール（`full‑web‑search`、`get‑web‑search‑summaries`、`get‑single‑web‑page‑content`）を提供します。API キー不要でページ内容を取得できるため、LLM が最新情報を利用できます。
- **Container Use** – coding agent 用の開発環境基盤。各エージェントに対して **独立したコンテナと Git ブランチから成る環境**を作成し、コマンド履歴やファイル変更履歴を完全に記録します。作業はメインのコードベースと分離され、ユーザーは `container-use log` や `diff` で結果を確認して「マージ」「適用」「破棄」を選択できます。
- **Ollama + gpt‑oss:20b** – オープンウエイトの大規模言語モデル。gpt‑oss‑20b は Apache2.0 ライセンスで提供され、16 GB メモリで動作可能なためローカル実行に適します。強力な推論性能と**ツール使用能力**（Web 検索や関数呼び出し）を備え、少数ショットでの関数呼び出しやチェイン・オブ・ソート推論に強みがあります。

## 2. 要件

### 2.1 機能要件

| 区分           | 要件内容 |
|----------------|---------|
| **チャットUI** | ユーザーが日本語・英語で対話できるモダンなチャットUI。ChatGPT に近い操作感を提供し、テキスト入力・ファイルアップロード・スケジュール予約・モデル切り替えなどを行える。 |
| **ログイン機能** | メールアドレス・ユーザー名・パスワードによる登録とログイン。パスワードはハッシュ化して PostgreSQL に保存。セッション管理は JWT などを使用。 |
| **ファイル取り込み** | PDF ファイルをアップロードし、コンテナ内で OCR/PDF パーサーを使ってテキスト抽出を行う。抽出後は要約やキーポイントの抽出・ハイライトを実行。 |
| **タスク予約・通知** | 指定した日時に要約タスクを実行できる予約機能。バックグラウンドで処理し、完了やエラーをチャット上とメールで通知。 |
| **検索支援** | Web‑Search‑MCP の `full‑web‑search` や `get‑web‑search‑summaries` を使い、論文の関連研究や背景情報を検索。抽出した全文を LLM の解析に取り込む。 |
| **エラー再実行** | パイプラインの各工程に監視機構を置き、失敗した場合は再試行を行い、それでも失敗する場合はエラー内容をユーザーに通知。 |
| **モデル切替** | オーラマで利用可能な複数モデルから選択可能 (例: gpt‑oss‑20b、Qwen3 など)。tool 呼び出し可能なモデルに限定。 |
| **日本語/英語対応** | 入出力言語を自動判定し、ユーザー指定により切り替え。日本語の文書も正確に解析・翻訳して要約できるよう翻訳ツールを組み込む。 |
| **データ管理** | PostgreSQL 上にユーザー、タスク、ファイル、チャットメッセージ、要約結果のテーブルを用意。 |
| **ローカル処理** | 一般的なクラウドモデル (OpenAI API, Claude, Google, Azure, AWS など) は使用せず、LLM や検索・解析はすべてローカルコンポーネントで実行。 |

### 2.2 非機能要件

- **セキュリティ**：通信は HTTPS。アップロードファイルの検査と拡張子制限。パスワードは強力なハッシュ (Argon2 など) で保存。ログイン試行回数の制限。
- **プライバシー保護**：PDF や会話内容は外部に送信せずローカル環境で処理。
- **拡張性**：マイクロサービス構成により機能追加やモデル追加が容易。Container Use を利用してエージェントを増やしても互いに干渉しない。
- **可用性**：エラー再実行機能やスケジュール実行により、長時間のタスクでも安定して処理。
- **保守性**：コンテナベースのデプロイと環境定義により再現性が高く、CI/CD に組み込みやすい。

## 3. システム構成

### 3.1 全体アーキテクチャ

```
┌─────────────┐  HTTP/WebSocket  ┌───────────────┐
│   フロントエンド │◄──────────────►│ バックエンドAPI │
└─────────────┘                      └───────────────┘
                                            │
                                    ┌─────────────┐
                                    │   DB (Postgres)│
                                    └─────────────┘
                                            │
    ┌─────────────┐        ┌───────────────┐         ┌─────────────┐
    │ Web‑Search‑MCP│◄───────│  タスクオーケストレータ │───────►│ Ollama (LLM) │
    └─────────────┘  MCP   └───────────────┘ HTTP    └─────────────┘
                                            │
                                    ┌─────────────┐
                                    │ Container Use│
                                    └─────────────┘
```

- **フロントエンド**：React などで構築し、ユーザー認証・チャット画面・ファイルアップロード・タスク予約 UI を提供。WebSocket でバックエンドとリアルタイム通信。
- **バックエンドAPI**：Python (FastAPI) や Node.js を用いた REST/WebSocket サーバ。ユーザー認証、タスク管理、LLM とのインターフェース、スケジューラ管理を担当。
- **DB (PostgreSQL)**：Docker で起動。ユーザー情報やタスク状態、メッセージ、ファイルメタデータ等を保存。
- **タスクオーケストレータ**：バックエンド内に組み込み、`Celery` や `APScheduler` 等のジョブキューでタスクを非同期に処理。予約時間になるとコンテナ内でPDF解析や要約処理を実行し、完了後チャットに結果を送信。
- **Web‑Search‑MCP サーバ**：外部Webの情報取得に使用。`full‑web‑search`はBing→Brave→DuckDuckGoの順に検索を行い、ページ全文を抽出する。軽量な`get‑web‑search‑summaries`は検索結果のスニペットのみ返す。
- **Container Use サーバ**：PDFパースや大きなモデル推論など、ファイル操作やコード実行を行う際に利用。各タスクごとに独立したコンテナ環境を作成し、メインのプロジェクトから隔離して処理できる。コンテナ環境にはベースイメージ（Ubuntu 24.04）や必要なPythonライブラリをプリインストール可能。
- **Ollama LLM サーバ**：gpt‑oss:20b や他のローカルモデルを提供。LLM は tool 呼び出し機能を持ち、Web‑Search‑MCP や Container Use を呼び出して分析を拡張できる。gpt‑oss は Apache2.0 ライセンスで強力なツール使用能力を持ち、16GB のメモリでローカル実行が可能。

### 3.2 データベース設計（主要テーブル）

| テーブル | 主なカラム | 役割 |
|----------|-----------|------|
| `users` | `id`, `email`, `username`, `password_hash`, `created_at`, `last_login` | ユーザー認証情報を保持。 |
| `tasks` | `id`, `user_id`, `task_type`(summary/search)、`file_path`, `model_name`, `schedule_time`, `status` (pending/running/success/fail)、`result_id`, `error_msg`, `created_at` | 要約・検索などのタスク情報。スケジュール実行に用いる。 |
| `files` | `id`, `user_id`, `file_name`, `storage_path`, `uploaded_at` | アップロードファイルのメタデータ。実ファイルはストレージに保存。 |
| `task_results` | `id`, `task_id`, `summary_text`, `analysis_text`, `created_at` | 要約や分析結果を格納。 |
| `messages` | `id`, `task_id`, `sender`(user/system), `content`, `timestamp` | チャットログ。 |

## 4. 主要コンポーネント設計

### 4.1 チャットフロントエンド

- **技術スタック**：React + TypeScript + Tailwind CSS。日本語 / 英語の i18n ライブラリを導入。
- **機能**
  - 認証画面：新規登録とログイン。
  - メイン画面：左側に会話リストとスケジュール一覧、右側にチャットビュー。ファイルアップロードボタンやモデル選択ドロップダウンを配置。
  - スケジュール設定：日時ピッカーでタスク予約。繰り返し設定などの拡張も検討。
  - 通知：WebSocket でタスク完了やエラーの通知をリアルタイムに受信し、トースト表示。

### 4.2 バックエンド API サーバ

- **エンドポイント例**
  - `POST /api/register`、`POST /api/login` – ユーザー登録・ログイン。
  - `POST /api/file` – PDF ファイルアップロード。ファイルを保存し、`files`テーブルに記録。
  - `POST /api/task/summary` – 即時実行の要約タスクを生成。必要に応じて後述のスケジューラを通じて非同期実行。
  - `POST /api/task/schedule` – 指定日時でタスクを予約。スケジューラに登録。
  - `GET /api/task/:id` – タスクの状態と結果を取得。
  - `GET /api/models` – 利用可能なローカルモデル一覧を取得。Ollama の `ollama list` コマンド等を呼び出して取得。

- **スケジューラ**
  - `APScheduler`（Python）や `BullMQ`（Node.js）などを使用して予約タスクを管理。`schedule_time` に基づきジョブを起動。
  - ジョブ実行時は Container Use で環境を作成し、以下のワークフローを実行:
    1. **PDF解析**：コンテナ内で `pdftotext` や `PyMuPDF` でページごとにテキスト抽出し、長文の場合はセクションごとに分割。
    2. **要約生成**：LLM（Ollama）に抽出テキストを入力し、要約・キーポイント・重要引用などを生成。必要に応じて Web‑Search‑MCP で関連情報を取得し、LLM で統合。
    3. **分析出力**：要約に加え、関連研究との比較や提案を含む分析を行う。分析手法はユーザー指定がない場合、LLM に「この論文の新規性・強み・課題」を指示するプロンプトを自動付与。
    4. **結果保存**：生成された要約・分析を `task_results` に保存。タスクの `status` を `success` に更新。
    5. **通知**：WebSocket 経由でユーザーに通知し、必要ならメール送信。

- **エラー処理**
  - コンテナでの実行エラー（ライブラリのインストール失敗、PDF 読み込み失敗など）は検出後、最大 N 回までリトライ。
  - それでも失敗する場合はタスク状態を `fail` に更新し、エラーメッセージを `error_msg` に保存。ユーザーへ通知。

### 4.3 Web‑Search‑MCP との連携

- 要約や分析時に関連研究を探索するため、LLM から tool 呼び出し機能を通して `full‑web‑search` を実行します。`full‑web‑search` は **Bing → Brave → DuckDuckGo** の順に検索し、各検索結果からページ全体を抽出。LLM には検索結果リストとページ内容を渡し、必要な情報を抽出させます。
- 軽量な調査の場合は `get‑web‑search‑summaries` を使用し、検索結果のスニペットのみで概要を掴ませます。
- URL の単独取得が必要な場合やユーザーがリンクを指定した場合は `get‑single‑web‑page‑content` を使用し、広告やナビゲーションを除いた本文のみを抽出できます。
- MCP ツール呼び出しは gpt‑oss の agentic 仕様に基づいて行われ、LLM が自律的に検索タスクを実行するためユーザーはキーワードを入力するだけで済みます。

### 4.4 Container Use の活用

- **環境分離**：各タスクは独立した環境（コンテナ + Git ブランチ）で実行され、メインプロジェクトのコードや設定に影響を与えません。
- **履歴追跡**：コンテナ内での全てのコマンドやファイル操作はログとして残り、`container-use log` や `container-use diff` で確認できます。ユーザーはエージェントの処理を透明に追跡可能。
- **環境設定**：デフォルトでは Ubuntu 24.04 が使用され、`container-use config` コマンドでベースイメージやインストールコマンド、環境変数を設定できます。例えば `python:3.11` や `build-essential` を追加インストールして PDF 解析ライブラリを準備できます。
- **安全な並列処理**：複数タスクを同時に実行しても、それぞれの環境が独立しているため競合や依存関係の衝突が起こりません。環境は使用後削除でき、再利用も可能です。

### 4.5 LLM インタフェース（Ollama）

- **モデル管理**：バックエンドは Ollama の API を利用し、現在ロード済みモデル一覧 (`ollama list`) を取得してフロントエンドへ返します。モデルのダウンロードや初期読み込みはシステム起動時に行い、必要に応じて追加モデルを準備。gpt‑oss‑20b の他に Qwen3 や Gemma 3 など tool 対応モデルも検討します。
- **プロンプト設計**：長文要約の場合はチャンク単位で LLM を呼び出し、要約後に全体統合を行う。要約だけでなく、新規性・問題点・関連研究といった分析項目を明示したプロンプトテンプレートを用意し、日本語・英語で出力させる。
- **ツール呼び出し**：gpt‑oss は Web 検索や Python 実行などのツール呼び出し機能をネイティブにサポートしており、`web‑search‑mcp` や `container-use` ツールを使った調査・計算を自律的に行えます。必要に応じて reasoning effort（低／中／高）を調整し、処理時間と精度を最適化します。

## 5. 処理フロー

以下はユーザーが PDF をアップロードし、要約を得るまでの代表的な処理フローです。

1. **ユーザー認証**：ユーザーはメールアドレスとパスワードでログイン。成功すると JWT が発行される。
2. **ファイルアップロード**：フロントエンドから PDF ファイルが `POST /api/file` に送信される。バックエンドはファイルをストレージに保存し、`files` テーブルに登録。
3. **タスク作成**：ユーザーが要約を依頼すると `POST /api/task/summary` が呼び出され、`tasks` レコードが生成される。即時実行の場合はキューに投入し、予約の場合はスケジューラに登録。
4. **環境セットアップ**：タスクワーカーは Container Use を用いて新しい環境を作成。デフォルトイメージに必要な Python ライブラリ (`pdfminer.six`, `langchain`, `ragas` など) をセットアップ。
5. **PDF解析**：環境内で PDF をテキスト化。ページ長が長い場合はセクション毎にチャンク分割。
6. **外部情報の取得 (任意)**：LLM が要約に必要な背景情報を得るため `full‑web‑search` を呼び出し、関連するウェブページの全文を取得。LLM はその内容も考慮して分析する。
7. **要約生成**：LLM にプロンプトとテキストチャンクを渡して要約を生成。結果を統合し、全体の要約・キーポイント・新規性分析を得る。
8. **結果保存と通知**：要約と分析を `task_results` テーブルに保存。タスク状態を success に変更。フロントエンドへ通知し、チャットに結果を表示。失敗した場合は error 状態とエラーメッセージを保存し通知する。

## 6. 実装上の留意点

- **大規模文書対応**：LLM のコンテキスト長を超える場合、チャンクごとにサマリーを作成し、メタ要約を生成する階層型要約を実装する。
- **日本語要約**：PDF が日本語の場合は日本語のまま要約を生成し、必要に応じて英訳を行う。LLM に対するプロンプトも言語を指定する。
- **GPU 資源管理**：gpt‑oss:20b は 16 GB メモリで動作可能とはいえ、複数コンテナで並列実行すると GPU が逼迫するため、実行キューを制限するかタスクごとにオプションで reasoning effort を下げる。
- **検索結果の信頼性**：web‑search‑mcp は各検索エンジンの結果を統合し、コンテンツ抽出の際に human behavior simulation によるアクセスを行うなど多層的な戦略を採用している。しかし広告や誤情報が含まれる場合もあるため、LLM に cross‑check を促すプロンプトを追加し、複数ソースの一致を評価させる。
- **タスクキャンセル**：ユーザーが長時間タスクをキャンセルできるようにし、タスクワーカーが実行中のコンテナを停止・削除するコマンドを提供する。
- **拡張性**：将来的に論文検索 API との連携や、他形式（Word, HTML）への対応、要約結果のエクスポート（Markdown/HTML）などを追加できるよう設計をモジュール化する。

## 7. おわりに

本システムは、Web‑Search‑MCP の強力なマルチエンジン検索、Container Use の安全な環境分離、そして gpt‑oss:20b の高性能なツール使用能力を組み合わせ、開発者・研究者がローカル環境で機密性を保ちつつ文献要約・分析を行うための基盤を提供します。タスク予約機能やモデル選択、エラー再実行などの要件を満たすことで、生産性を高めつつ運用の手間を減らせる設計としました。今後はユーザーからのフィードバックを反映し、分析精度やユーザー体験の向上に努めます。

